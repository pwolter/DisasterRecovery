{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pwolter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pwolter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/pwolter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet']);\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///emergency_response.db')\n",
    "conn = engine.connect()\n",
    "df = pd.read_sql_table('messages', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10% of dataframe to speed up modeling - change to 100% for full run\n",
    "sample_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting cross validation folds\n",
    "cv = 5\n",
    "\n",
    "# random state \n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for the file to contain the model to save\n",
    "file_name = 'model/classifier.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=sample_rate, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1311, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                        False\n",
       "message                   False\n",
       "original                  False\n",
       "genre                     False\n",
       "related                   False\n",
       "request                   False\n",
       "offer                     False\n",
       "aid_related               False\n",
       "medical_help              False\n",
       "medical_products          False\n",
       "search_and_rescue         False\n",
       "security                  False\n",
       "military                  False\n",
       "child_alone                True\n",
       "water                     False\n",
       "food                      False\n",
       "shelter                   False\n",
       "clothing                  False\n",
       "money                     False\n",
       "missing_people            False\n",
       "refugees                  False\n",
       "death                     False\n",
       "other_aid                 False\n",
       "infrastructure_related    False\n",
       "transport                 False\n",
       "buildings                 False\n",
       "electricity               False\n",
       "tools                     False\n",
       "hospitals                 False\n",
       "shops                     False\n",
       "aid_centers               False\n",
       "other_infrastructure      False\n",
       "weather_related           False\n",
       "floods                    False\n",
       "storm                     False\n",
       "fire                      False\n",
       "earthquake                False\n",
       "cold                      False\n",
       "other_weather             False\n",
       "direct_report             False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df == 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete child_alone and shops also as all are 0s\n",
    "columns = ['id', 'message', 'original', 'genre', 'request', 'offer', 'child_alone', 'shops']\n",
    "\n",
    "X = df['message']\n",
    "Y = df.drop(columns=columns)\n",
    "\n",
    "category_names = Y.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframes to numpy arrays\n",
    "# X = X.values\n",
    "# Y = Y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1311,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1311, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    # URL replacement\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    # Lowercase and punctuation removal\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()).split()\n",
    "\n",
    "    # Stop word removal\n",
    "    words = [w for w in text if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    # Stemming\n",
    "    stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w) for w in stemmed]\n",
    "\n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(983,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(983, 32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the Random Forest Classifier (rfc) pipeline\n",
    "rfc_pipeline = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=random_state)))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.48 s, sys: 901 ms, total: 5.38 s\n",
      "Wall time: 5.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "           n_jobs=None))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fitting the rfc classifier with the baseline pipeline\n",
    "\n",
    "rfc_pipeline.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 929 ms, sys: 273 ms, total: 1.2 s\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# predicting\n",
    "\n",
    "y_pred = rfc_pipeline.predict(X_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18902439024390244\n"
     ]
    }
   ],
   "source": [
    "# printing the score\n",
    "print(rfc_pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test models based on Udacity's provided ocde\n",
    "def test_model(y_test, y_pred, category_names):\n",
    "    print(classification_report(y_test, y_pred, target_names=category_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.76      0.82      0.79       237\n",
      "           aid_related       0.73      0.55      0.63       130\n",
      "          medical_help       0.00      0.00      0.00        30\n",
      "      medical_products       0.00      0.00      0.00        25\n",
      "     search_and_rescue       0.00      0.00      0.00         2\n",
      "              security       0.00      0.00      0.00         4\n",
      "              military       0.00      0.00      0.00         7\n",
      "                 water       0.44      0.21      0.29        19\n",
      "                  food       0.93      0.68      0.79        38\n",
      "               shelter       1.00      0.12      0.21        26\n",
      "              clothing       0.00      0.00      0.00         3\n",
      "                 money       0.00      0.00      0.00         8\n",
      "        missing_people       0.00      0.00      0.00         5\n",
      "              refugees       1.00      0.08      0.15        12\n",
      "                 death       0.00      0.00      0.00        17\n",
      "             other_aid       0.00      0.00      0.00        39\n",
      "infrastructure_related       0.00      0.00      0.00        22\n",
      "             transport       0.00      0.00      0.00        21\n",
      "             buildings       1.00      0.10      0.17        21\n",
      "           electricity       0.00      0.00      0.00         6\n",
      "                 tools       0.00      0.00      0.00         1\n",
      "             hospitals       0.00      0.00      0.00         4\n",
      "           aid_centers       0.00      0.00      0.00         4\n",
      "  other_infrastructure       0.00      0.00      0.00        16\n",
      "       weather_related       0.74      0.39      0.51        89\n",
      "                floods       0.00      0.00      0.00        31\n",
      "                 storm       0.50      0.04      0.08        24\n",
      "                  fire       0.00      0.00      0.00         2\n",
      "            earthquake       1.00      0.31      0.48        32\n",
      "                  cold       0.00      0.00      0.00         3\n",
      "         other_weather       0.00      0.00      0.00        18\n",
      "         direct_report       0.75      0.20      0.32        60\n",
      "\n",
      "             micro avg       0.75      0.38      0.50       956\n",
      "             macro avg       0.28      0.11      0.14       956\n",
      "          weighted avg       0.56      0.38      0.41       956\n",
      "           samples avg       0.57      0.34      0.38       956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pwolter/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/pwolter/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/pwolter/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# report of classification results\n",
    "test_model(y_test, y_pred, category_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding Vectorizer and TF-IDF parameters to GridSearch\n",
    "parameters_rfc = {\n",
    "    'tfidf__use_idf': (True, False),\n",
    "\n",
    "    'clf__estimator__n_estimators': (50, 100, 150),\n",
    "    'clf__estimator__min_samples_leaf': (2, 3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.8 s, sys: 1.1 s, total: 6.9 s\n",
      "Wall time: 4min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "           n_jobs=None))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'tfidf__use_idf': (True, False), 'clf__estimator__n_estimators': (50, 100, 150), 'clf__estimator__min_samples_leaf': (2, 3)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# running GridSearch and fitting the model\n",
    "\n",
    "grid_search_rfc = GridSearchCV(estimator=rfc_pipeline, param_grid=parameters_rfc, n_jobs=-1, cv=cv,\n",
    "                           refit=True, return_train_score=True, verbose=1)\n",
    "grid_search_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x1a1ce9eef0>, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False,\n",
      "         use_idf=False)), ('clf', MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
      "           n_jobs=None))], 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x1a1ce9eef0>, vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False,\n",
      "         use_idf=False), 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
      "           n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x1a1ce9eef0>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': False, 'clf__estimator__bootstrap': True, 'clf__estimator__class_weight': None, 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 2, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 50, 'clf__estimator__n_jobs': None, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': 42, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False), 'clf__n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "# Best parameters for rfc\n",
    "best_parameters_rfc = grid_search_rfc.best_estimator_.get_params()\n",
    "print(best_parameters_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the best model found\n",
    "best_model_rfc = grid_search_rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
      "           n_jobs=None))])\n"
     ]
    }
   ],
   "source": [
    "# printing the best estimator found\n",
    "print(grid_search_rfc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16581892166836215\n"
     ]
    }
   ],
   "source": [
    "# printing the score to compare to previous model --> 0.21392190152801357\n",
    "print(grid_search_rfc.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model_rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.74      0.98      0.85       237\n",
      "           aid_related       0.65      0.67      0.66       130\n",
      "          medical_help       1.00      0.03      0.06        30\n",
      "      medical_products       0.00      0.00      0.00        25\n",
      "     search_and_rescue       0.00      0.00      0.00         2\n",
      "              security       0.00      0.00      0.00         4\n",
      "              military       0.00      0.00      0.00         7\n",
      "                 water       0.00      0.00      0.00        19\n",
      "                  food       1.00      0.21      0.35        38\n",
      "               shelter       1.00      0.04      0.07        26\n",
      "              clothing       0.00      0.00      0.00         3\n",
      "                 money       0.00      0.00      0.00         8\n",
      "        missing_people       0.00      0.00      0.00         5\n",
      "              refugees       0.00      0.00      0.00        12\n",
      "                 death       0.00      0.00      0.00        17\n",
      "             other_aid       0.00      0.00      0.00        39\n",
      "infrastructure_related       0.00      0.00      0.00        22\n",
      "             transport       0.00      0.00      0.00        21\n",
      "             buildings       0.00      0.00      0.00        21\n",
      "           electricity       0.00      0.00      0.00         6\n",
      "                 tools       0.00      0.00      0.00         1\n",
      "             hospitals       0.00      0.00      0.00         4\n",
      "           aid_centers       0.00      0.00      0.00         4\n",
      "  other_infrastructure       0.00      0.00      0.00        16\n",
      "       weather_related       0.81      0.54      0.65        89\n",
      "                floods       0.81      0.42      0.55        31\n",
      "                 storm       1.00      0.12      0.22        24\n",
      "                  fire       0.00      0.00      0.00         2\n",
      "            earthquake       1.00      0.38      0.55        32\n",
      "                  cold       0.00      0.00      0.00         3\n",
      "         other_weather       0.00      0.00      0.00        18\n",
      "         direct_report       0.69      0.15      0.25        60\n",
      "\n",
      "             micro avg       0.74      0.43      0.55       956\n",
      "             macro avg       0.27      0.11      0.13       956\n",
      "          weighted avg       0.58      0.43      0.44       956\n",
      "           samples avg       0.66      0.42      0.47       956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pwolter/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/pwolter/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/pwolter/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "test_model(y_test, y_pred, category_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and pipeline for Adaboost classifier\n",
    "parameters_ab = {\n",
    "    'tfidf__use_idf': (True, False),\n",
    "\n",
    "    'clf__estimator__n_estimators': (50, 100, 150),\n",
    "    'clf__estimator__learning_rate': (0.1, 0.15, 0.2),\n",
    "}\n",
    "\n",
    "ab_pipeline = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(AdaBoostClassifier(random_state=random_state)))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  6.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.8 s, sys: 919 ms, total: 7.72 s\n",
      "Wall time: 6min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "           n_jobs=None))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'tfidf__use_idf': (True, False), 'clf__estimator__n_estimators': (50, 100, 150), 'clf__estimator__learning_rate': (0.1, 0.15, 0.2)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# run gridserach and fit the model\n",
    "\n",
    "grid_search_ada = GridSearchCV(estimator=ab_pipeline, param_grid=parameters_ab, n_jobs=-1, cv=cv,\n",
    "                           refit=True, return_train_score=True, verbose=1)\n",
    "grid_search_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x1a1ce9eef0>, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False,\n",
      "         use_idf=False)), ('clf', MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.1, n_estimators=50, random_state=42),\n",
      "           n_jobs=None))], 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x1a1ce9eef0>, vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False,\n",
      "         use_idf=False), 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.1, n_estimators=50, random_state=42),\n",
      "           n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x1a1ce9eef0>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': False, 'clf__estimator__algorithm': 'SAMME.R', 'clf__estimator__base_estimator': None, 'clf__estimator__learning_rate': 0.1, 'clf__estimator__n_estimators': 50, 'clf__estimator__random_state': 42, 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.1, n_estimators=50, random_state=42), 'clf__n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "# save best estimator and print best parameters found\n",
    "best_parameters_ada = grid_search_ada.best_estimator_.get_params()\n",
    "print(best_parameters_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...ator=None,\n",
      "          learning_rate=0.1, n_estimators=50, random_state=42),\n",
      "           n_jobs=None))])\n"
     ]
    }
   ],
   "source": [
    "# save best model\n",
    "best_model_ada = grid_search_ada.best_estimator_\n",
    "\n",
    "print(grid_search_ada.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20040691759918616\n"
     ]
    }
   ],
   "source": [
    "# score for adaboost --> 0.22\n",
    "print(grid_search_ada.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with ada best model\n",
    "y_pred = best_model_ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.72      1.00      0.84       237\n",
      "           aid_related       0.75      0.33      0.46       130\n",
      "          medical_help       0.50      0.07      0.12        30\n",
      "      medical_products       0.67      0.08      0.14        25\n",
      "     search_and_rescue       0.00      0.00      0.00         2\n",
      "              security       0.00      0.00      0.00         4\n",
      "              military       0.00      0.00      0.00         7\n",
      "                 water       0.93      0.74      0.82        19\n",
      "                  food       0.83      0.66      0.74        38\n",
      "               shelter       0.73      0.31      0.43        26\n",
      "              clothing       1.00      0.33      0.50         3\n",
      "                 money       0.00      0.00      0.00         8\n",
      "        missing_people       0.67      0.40      0.50         5\n",
      "              refugees       0.00      0.00      0.00        12\n",
      "                 death       0.00      0.00      0.00        17\n",
      "             other_aid       0.00      0.00      0.00        39\n",
      "infrastructure_related       0.00      0.00      0.00        22\n",
      "             transport       0.67      0.10      0.17        21\n",
      "             buildings       1.00      0.05      0.09        21\n",
      "           electricity       0.00      0.00      0.00         6\n",
      "                 tools       0.00      0.00      0.00         1\n",
      "             hospitals       0.00      0.00      0.00         4\n",
      "           aid_centers       0.00      0.00      0.00         4\n",
      "  other_infrastructure       0.20      0.06      0.10        16\n",
      "       weather_related       0.79      0.58      0.67        89\n",
      "                floods       0.83      0.48      0.61        31\n",
      "                 storm       0.50      0.29      0.37        24\n",
      "                  fire       0.00      0.00      0.00         2\n",
      "            earthquake       0.91      0.66      0.76        32\n",
      "                  cold       1.00      0.33      0.50         3\n",
      "         other_weather       0.00      0.00      0.00        18\n",
      "         direct_report       0.82      0.15      0.25        60\n",
      "\n",
      "             micro avg       0.74      0.46      0.57       956\n",
      "             macro avg       0.42      0.21      0.25       956\n",
      "          weighted avg       0.63      0.46      0.48       956\n",
      "           samples avg       0.68      0.44      0.49       956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pwolter/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/pwolter/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/pwolter/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# report of classification results\n",
    "test_model(y_test, y_pred, category_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model to use it in the app\n",
    "# with open(file_name, 'wb') as pickled_model:\n",
    "#     pickle.dump(best_model, pickled_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
